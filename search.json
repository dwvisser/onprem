[
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Examples",
    "section": "",
    "text": "In these examples, we will accelerate inference using a GPU. We use an NVIDIA Titan V GPU with a modest 12GB of VRAM. For GPU acceleration, make sure you installed llama-cpp-python with CUBLAS support:\nAfter that, you just need to supply the n_gpu_layers argument to LLM for GPU-accelerated responses."
  },
  {
    "objectID": "examples.html#setup-the-llm-instance",
    "href": "examples.html#setup-the-llm-instance",
    "title": "Examples",
    "section": "Setup the LLM instance",
    "text": "Setup the LLM instance\n\nfrom onprem import LLM\nimport os\n\nllm = LLM(n_gpu_layers=35)"
  },
  {
    "objectID": "examples.html#information-extraction",
    "href": "examples.html#information-extraction",
    "title": "Examples",
    "section": "Information Extraction",
    "text": "Information Extraction\n\nprompt = \"\"\"Extract the names of people in the supplied sentences. Here is an example:\nSentence: James Gandolfini and Paul Newman were great actors.\nPeople:\nJames Gandolfini, Paul Newman\nSentence:\nI like Cillian Murphy's acting. Florence Pugh is great, too.\nPeople:\"\"\"\n\nsaved_output = llm.prompt(prompt)\n\n\nCillian Murphy, Florence Pugh\n\n\nA more complicated example of Information Extraction:\n\nprompt = \"\"\"Extract the Name, Position, and Company from the following sentences.  Here are some examples.\n[Text]: Fred is a serial entrepreneur. Co-founder and CEO of Platform.sh, he previously co-founded Commerce Guys, a leading Drupal ecommerce provider. His mission is to guarantee that as we continue on an ambitious journey to profoundly transform how cloud computing is used and perceived, we keep our feet well on the ground continuing the rapid growth we have enjoyed up until now. \n[Name]: Fred\n[Position]: Co-founder and CEO\n[Company]: Platform.sh\n###\n[Text]: Microsoft (the word being a portmanteau of \"microcomputer software\") was founded by Bill Gates on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800. Steve Ballmer replaced Gates as CEO in 2000, and later envisioned a \"devices and services\" strategy.\n[Name]:  Steve Ballmer\n[Position]: CEO\n[Company]: Microsoft\n###\n[Text]: Franck Riboud was born on 7 November 1955 in Lyon. He is the son of Antoine Riboud, the previous CEO, who transformed the former European glassmaker BSN Group into a leading player in the food industry. He is the CEO at Danone.\n[Name]:  Franck Riboud\n[Position]: CEO\n[Company]: Danone\n###\n[Text]: David Melvin is an investment and financial services professional at CITIC CLSA with over 30 years’ experience in investment banking and private equity. He is currently a Senior Adviser of CITIC CLSA.\n\"\"\"\nsaved_output = llm.prompt(prompt)\n\n[Name]:  David Melvin\n[Position]: Senior Adviser\n[Company]: CITIC CLSA"
  },
  {
    "objectID": "examples.html#grammar-correction",
    "href": "examples.html#grammar-correction",
    "title": "Examples",
    "section": "Grammar Correction",
    "text": "Grammar Correction\n\nprompt=\"\"\"Correct the grammar and spelling in the supplied sentences.  Here are some examples.\n[Sentence]:\nI love goin to the beach.\n[Correction]: I love going to the beach.\n[Sentence]:\nLet me hav it!\n[Correction]: Let me have it!\n[Sentence]:\nIt have too many drawbacks.\n[Correction]: It has too many drawbacks.\n[Sentence]:\nI do not wan to go\n[Correction]:\"\"\"\nsaved_output = llm.prompt(prompt)\n\n I don't want to go."
  },
  {
    "objectID": "examples.html#classification",
    "href": "examples.html#classification",
    "title": "Examples",
    "section": "Classification",
    "text": "Classification\n\nprompt=\"\"\"Classify each sentence as either positive, negative, or neutral.  Here are some examples.\n[Sentence]: I love going to the beach.\n[[Classification]: Positive\n[Sentence]: It is 10am right now.\n[Classification]: Neutral\n[Sentence]: I just got fired from my job.\n[Classification]: Negative\n[Sentence]: The reactivity of  your team has been amazing, thanks!\n[Classification]:\"\"\"\n\nsaved_output = llm.prompt(prompt)\n\n Positive"
  },
  {
    "objectID": "examples.html#paraphrasing",
    "href": "examples.html#paraphrasing",
    "title": "Examples",
    "section": "Paraphrasing",
    "text": "Paraphrasing\n\nprompt = \"\"\"Paraphrase the following text delimited by triple backticks. \n```After a war lasting 20 years, following the decision taken first by President Trump and then by President Biden to withdraw American troops, Kabul, the capital of Afghanistan, fell within a few hours to the Taliban, without resistance.```\n\"\"\"\nsaved_output = llm.prompt(prompt)\n\nParaphrase: Following a prolonged war in Afghanistan that lasted for 20 years, when both U.S. Presidents Trump and Biden decided to withdraw American troops from the country, Kabul - the capital city of Afghanistan - fell within hours into the hands of the Taliban without any resistance."
  },
  {
    "objectID": "examples.html#few-shot-answer-extraction",
    "href": "examples.html#few-shot-answer-extraction",
    "title": "Examples",
    "section": "Few-Shot Answer Extraction",
    "text": "Few-Shot Answer Extraction\n\nprompt=\"\"\"Answer the Question based on the Context.  Here are some examples.\n[Context]: \nNLP Cloud was founded in 2021 when the team realized there was no easy way to reliably leverage Natural Language Processing in production.\nQuestion: When was NLP Cloud founded?\n[Answer]: \n2021\n###\n[Context]:\nNLP Cloud developed their API by mid-2020 and they added many pre-trained open-source models since then.\n[Question]: \nWhat did NLP Cloud develop?\n[Answer]:\nAPI\n###\n[Context]:\nAll plans can be stopped anytime. You only pay for the time you used the service. In case of a downgrade, you will get a discount on your next invoice.\n[Question]:\nWhen can plans be stopped?\n[Answer]:\nAnytime\n###\n[Context]:\nThe main challenge with GPT-J is memory consumption. Using a GPU plan is recommended.\n[Question]:\nWhich plan is recommended for GPT-J?\nAnswer:\"\"\"\nsaved_output = llm.prompt(prompt)\n\n \nGPU Plan"
  },
  {
    "objectID": "examples.html#generating-product-descriptions",
    "href": "examples.html#generating-product-descriptions",
    "title": "Examples",
    "section": "Generating Product Descriptions",
    "text": "Generating Product Descriptions\n\nprompt=\"\"\"Generate a Sentence from the Keywords. Here are some examples.\n[Keywords]:\nshoes, women, $59\n[Sentence]:\nBeautiful shoes for women at the price of $59.\n###\n[Keywords]:\ntrousers, men, $69\n[Sentence]:\nModern trousers for men, for $69 only.\n###\n[Keywords]:\ngloves, winter, $19\n[Sentence]: \nAmazingly hot gloves for cold winters, at $19.\n###\n[Keywords]: \nt-shirt, men, $39\n[Sentence]:\"\"\"\nsaved_output = llm.prompt(prompt)\n\n\nA stylish and comfortable t-shirt for men, for just $39."
  },
  {
    "objectID": "examples.html#tweet-generation",
    "href": "examples.html#tweet-generation",
    "title": "Examples",
    "section": "Tweet Generation",
    "text": "Tweet Generation\n\nprompt = \"\"\"Generate a tweet based on the supplied Keyword. Here are some examples.\n[Keyword]:\nmarkets\n[Tweet]:\nTake feedback from nature and markets, not from people\n###\n[Keyword]:\nchildren\n[Tweet]:\nMaybe we die so we can come back as children.\n###\n[Keyword]:\nstartups\n[Tweet]: \nStartups should not worry about how to put out fires, they should worry about how to start them.\n###\n[Keyword]:\nclimate change\n[Tweet]:\"\"\"\nsaved_output = llm.prompt(prompt)\n\n\nThe climate is changing, but our actions can save it from catastrophe.\n###"
  },
  {
    "objectID": "examples.html#drafting-emails",
    "href": "examples.html#drafting-emails",
    "title": "Examples",
    "section": "Drafting Emails",
    "text": "Drafting Emails\n\nprompt = \"\"\"Generate an email introducing Tesla to shareholders.\"\"\"\nsaved_output = llm.prompt(prompt)\n\n\nSubject: Introducing Tesla, the future of transportation\nDear Shareholder,\nI am writing to introduce you to Tesla, the leading manufacturer of electric vehicles (EVs) and energy storage products. \nTesla has been at the forefront of innovating EV technology since its inception in 2003. With a history of creating breakthrough products such as the Tesla Model S, Tesla Powerwall, and Solar Roof, it is clear that Tesla's mission is to create a better future through sustainable transportation and energy solutions.\nTesla has experienced rapid growth in recent years, with its stock price increasing by 475% since January 2016. This growth can be attributed to several factors, including the increase in global interest in sustainability and environmental issues, as well as Tesla's continuous innovation and success in the EV market.\nOverall, Tesla is a company that not only produces cutting-edge products but also has a strong commitment to sustainability and innovation. As a shareholder of Tesla, I encourage you to take advantage of this opportunity to invest in the future of transportation and energy storage solutions with Tesla.\nThank you for your continued support of Tesla."
  },
  {
    "objectID": "examples.html#talk-to-your-documents",
    "href": "examples.html#talk-to-your-documents",
    "title": "Examples",
    "section": "Talk to Your Documents",
    "text": "Talk to Your Documents\nNote: Although we are using the default 7B-parameter model in the example below, we recommend using a larger model for retrieval-augmented question-answering use cases like this in order to reduce hallucinations. You can use the default 13B-parameter model by supplying use_larger=True in the call to onprem.LLM.\n\nllm.ingest('./sample_data/')\n\n2023-09-05 14:47:43.265584: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nLoading new documents: 100%|██████████████████████| 2/2 [00:00&lt;00:00, 16.59it/s]\n\n\nCreating new vectorstore\nLoading documents from ./sample_data/\nLoaded 11 new documents from ./sample_data/\nSplit into 62 chunks of text (max. 500 tokens each)\nCreating embeddings. May take some minutes...\nIngestion complete! You can now query your documents using the LLM.ask method\n\n\n\nanswer, sources = llm.ask('What is ktrain? Remember to only use the provided context.')\n\n KTrain is a low-code library for augmented machine learning that automates or semi-automates various aspects of the ML workow, from data preprocessing to model training and application. It is inspired by other low-code and no-code open-source ML libraries such as fastai and ludwig, and is intended to help democratize machine learning by enabling beginners and domain experts with minimal programming or data science expertise.\n\n\nPro-Tip: If you see the model hallucinating answers, you can supply use_larger=True to LLM and use the slightly larger default model better-suited to this use case (or supply the URL to a different model of your choosing to LLM), which can provide better performance."
  },
  {
    "objectID": "ingest.html",
    "href": "ingest.html",
    "title": "ingest",
    "section": "",
    "text": "source\n\nMyElmLoader\n\n MyElmLoader (file_path:str, mode:str='single', **unstructured_kwargs:Any)\n\nWrapper to fallback to text/plain when default does not work\n\nsource\n\n\ndoes_vectorstore_exist\n\n does_vectorstore_exist (persist_directory:str,\n                         embeddings:langchain.embeddings.huggingface.Huggi\n                         ngFaceEmbeddings)\n\nChecks if vectorstore exists\n\nsource\n\n\nprocess_documents\n\n process_documents (source_directory:str, ignored_files:List[str]=[],\n                    chunk_size=500, chunk_overlap=50)\n\nLoad documents and split in chunks\nArgs:\n\nsource_directory: path to folder containing document store\nchunk_size: text is split to this many characters by langchain.text_splitter.RecursiveCharacterTextSplitter\nchunk_overlap: character overlap between chunks in langchain.text_splitter.RecursiveCharacterTextSplitter\n\nReturns: list of langchain.docstore.document.Document objects\n\nsource\n\n\nload_documents\n\n load_documents (source_dir:str, ignored_files:List[str]=[])\n\nLoads all documents from the source documents directory, ignoring specified files\n\nsource\n\n\nload_single_document\n\n load_single_document (file_path:str)\n\n\nsource\n\n\nIngester\n\n Ingester (embedding_model_name:str='sentence-transformers/all-\n           MiniLM-L6-v2', embedding_model_kwargs:dict={'device': 'cpu'},\n           persist_directory:Optional[str]=None)\n\nIngests all documents in source_folder (previously-ingested documents are ignored)\nArgs:\n\nembedding_model: name of sentence-transformers model\nembedding_model_kwargs: arguments to embedding model (e.g., {device':'cpu'})\npersist_directory: Path to vector database (created if it doesn’t exist). Default is onprem_data/vectordb in user’s home directory.\n\nReturns: None\n\nsource\n\n\nIngester.get_db\n\n Ingester.get_db ()\n\nReturns an instance to the langchain.vectorstores.Chroma instance\n\nsource\n\n\nIngester.ingest\n\n Ingester.ingest (source_directory:str, chunk_size:int=500,\n                  chunk_overlap:int=50)\n\nIngests all documents in source_directory (previously-ingested documents are ignored).\nArgs:\n\nsource_directory: path to folder containing document store\nchunk_size: text is split to this many characters by langchain.text_splitter.RecursiveCharacterTextSplitter\nchunk_overlap: character overlap between chunks in langchain.text_splitter.RecursiveCharacterTextSplitter\n\nReturns: None\n\ningester = Ingester()\ningester.ingest('sample_data')\n\nAppending to existing vectorstore at /home/amaiya/onprem_data/vectordb\nLoading documents from sample_data\nNo new documents to load\n\n\nLoading new documents: 0it [00:00, ?it/s]"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\nget_datadir\n\n get_datadir ()\n\n\nsource\n\n\ndownload\n\n download (url, filename, verify=False)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OnPrem.LLM",
    "section": "",
    "text": "A tool for running large language models on-premises using non-public data\nOnPrem.LLM is a simple Python package that makes it easier to run large language models (LLMs) on your own machines using non-public data (possibly behind corporate firewalls). Inspired by the privateGPT GitHub repo and Simon Willison’s LLM command-line utility, OnPrem.LLM is intended to help integrate local LLMs into practical applications.\nThe full documentation is here.\nA Google Colab demo of installing and using OnPrem.LLM is here."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "OnPrem.LLM",
    "section": "Install",
    "text": "Install\nOnce installing PyTorch, you can install OnPrem.LLM with:\npip install onprem\nFor fast GPU-accelerated inference, see additional instructions below."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "OnPrem.LLM",
    "section": "How to use",
    "text": "How to use\n\nSetup\n\nfrom onprem import LLM\n\nllm = LLM()\n\nBy default, a 7B-parameter model is downloaded and used. If use_larger=True, a 13B-parameter is used. You can also supply the URL to an LLM of your choosing to LLM (see the code generation section below for an example). Currently, only models in GGML format are supported. Future versions of OnPrem.LLM will transition to the newer GGUF format.\n\n\nSend Prompts to the LLM to Solve Problems\nThis is an example of few-shot prompting, where we provide an example of what we want the LLM to do.\n\nprompt = \"\"\"Extract the names of people in the supplied sentences. Here is an example:\nSentence: James Gandolfini and Paul Newman were great actors.\nPeople:\nJames Gandolfini, Paul Newman\nSentence:\nI like Cillian Murphy's acting. Florence Pugh is great, too.\nPeople:\"\"\"\n\nsaved_output = llm.prompt(prompt)\n\n\nCillian Murphy, Florence Pugh\n\n\nAdditional prompt examples are shown here.\n\n\nTalk to Your Documents\nAnswers are generated from the content of your documents.\n\nStep 1: Ingest the Documents into a Vector Database\n\nllm.ingest('./sample_data')\n\n2023-09-03 16:30:54.459509: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nLoading new documents: 100%|██████████████████████| 2/2 [00:00&lt;00:00, 17.16it/s]\n\n\nCreating new vectorstore\nLoading documents from ./sample_data\nLoaded 11 new documents from ./sample_data\nSplit into 62 chunks of text (max. 500 tokens each)\nCreating embeddings. May take some minutes...\nIngestion complete! You can now query your documents using the LLM.ask method\n\n\n\n\nStep 2: Answer Questions About the Documents\n\nquestion = \"\"\"What is  ktrain?\"\"\" \nanswer, docs = llm.ask(question)\nprint('\\n\\nReferences:\\n\\n')\nfor i, document in enumerate(docs):\n    print(f\"\\n{i+1}.&gt; \" + document.metadata[\"source\"] + \":\")\n    print(document.page_content)\n\n Ktrain is a low-code machine learning library designed to augment human\nengineers in the machine learning workow by automating or semi-automating various\naspects of model training, tuning, and application. Through its use, domain experts can\nleverage their expertise while still benefiting from the power of machine learning techniques.\n\nReferences:\n\n\n\n1.&gt; ./sample_data/ktrain_paper.pdf:\nlection (He et al., 2019). By contrast, ktrain places less emphasis on this aspect of au-\ntomation and instead focuses on either partially or fully automating other aspects of the\nmachine learning (ML) workﬂow. For these reasons, ktrain is less of a traditional Au-\n2\n\n2.&gt; ./sample_data/ktrain_paper.pdf:\npossible, ktrain automates (either algorithmically or through setting well-performing de-\nfaults), but also allows users to make choices that best ﬁt their unique application require-\nments. In this way, ktrain uses automation to augment and complement human engineers\nrather than attempting to entirely replace them. In doing so, the strengths of both are\nbetter exploited. Following inspiration from a blog post1 by Rachel Thomas of fast.ai\n\n3.&gt; ./sample_data/ktrain_paper.pdf:\nwith custom models and data formats, as well.\nInspired by other low-code (and no-\ncode) open-source ML libraries such as fastai (Howard and Gugger, 2020) and ludwig\n(Molino et al., 2019), ktrain is intended to help further democratize machine learning by\nenabling beginners and domain experts with minimal programming or data science experi-\n4. http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\n6\n\n4.&gt; ./sample_data/ktrain_paper.pdf:\nktrain: A Low-Code Library for Augmented Machine Learning\ntoML platform and more of what might be called a “low-code” ML platform. Through\nautomation or semi-automation, ktrain facilitates the full machine learning workﬂow from\ncurating and preprocessing inputs (i.e., ground-truth-labeled training data) to training,\ntuning, troubleshooting, and applying models. In this way, ktrain is well-suited for domain\nexperts who may have less experience with machine learning and software coding. Where\n\n\nPro-Tip: If you see the model hallucinating answers, you can supply use_larger=True to LLM and use the slightly larger default model better-suited to this use case (or supply the URL to a different model of your choosing to LLM), which can provide better performance.\n\n\n\nText to Code Generation\nWe’ll use the CodeUp LLM by supplying the URL and employing the particular prompt format this model expects.\n\nfrom onprem import LLM\nurl = 'https://huggingface.co/TheBloke/CodeUp-Llama-2-13B-Chat-HF-GGML/resolve/main/codeup-llama-2-13b-chat-hf.ggmlv3.q4_1.bin'\nllm = LLM(url, n_gpu_layers=43) # see below for GPU information\n\nSetup the prompt based on what this model expects (this is important):\n\ntemplate = \"\"\"\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{prompt}\n\n### Response:\"\"\"\n\n\nanswer = llm.prompt('Write Python code to validate an email address.', prompt_template=template)\n\n\nHere is an example of Python code that can be used to validate an email address:\n```\nimport re\n\ndef validate_email(email):\n    # Use a regular expression to check if the email address is in the correct format\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    if re.match(pattern, email):\n        return True\n    else:\n        return False\n\n# Test the validate_email function with different inputs\nprint(\"Email address is valid:\", validate_email(\"example@example.com\"))  # Should print \"True\"\nprint(\"Email address is invalid:\", validate_email(\"example@\"))  # Should print \"False\"\nprint(\"Email address is invalid:\", validate_email(\"example.com\"))  # Should print \"False\"\n```\nThe code defines a function `validate_email` that takes an email address as input and uses a regular expression to check if the email address is in the correct format. The regular expression checks for an email address that consists of one or more letters, numbers, periods, hyphens, or underscores followed by the `@` symbol, followed by one or more letters, periods, hyphens, or underscores followed by a `.` and two to three letters.\nThe function returns `True` if the email address is valid, and `False` otherwise. The code also includes some test examples to demonstrate how to use the function.\n\n\nLet’s try out the code generated above.\n\nimport re\ndef validate_email(email):\n    # Use a regular expression to check if the email address is in the correct format\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    if re.match(pattern, email):\n        return True\n    else:\n        return False\nprint(validate_email('sam@@openai.com')) # bad email address\nprint(validate_email('sam@openai'))      # bad email address\nprint(validate_email('sam@openai.com'))  # good email address\n\nFalse\nFalse\nTrue\n\n\nThe generated code may sometimes need editing, but this one worked out-of-the-box."
  },
  {
    "objectID": "index.html#speeding-up-inference-using-a-gpu",
    "href": "index.html#speeding-up-inference-using-a-gpu",
    "title": "OnPrem.LLM",
    "section": "Speeding Up Inference Using a GPU",
    "text": "Speeding Up Inference Using a GPU\nThe above example employed the use of a CPU.\nIf you have a GPU (even an older one with less VRAM), you can speed up responses.\n\nStep 1: Install llama-cpp-python with CUBLAS support\nCMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python==0.1.69 --no-cache-dir\nIt is important to use the specific version shown above due to library incompatibilities.\n\n\nStep 2: Use the n_gpu_layers argument with LLM\nllm = LLM(n_gpu_layers=35)\nThe value for n_gpu_layers depends on your GPU memory and the model you’re using (e.g., max of 35 for default 7B model). You can reduce the value if you get an error (e.g., CUDA OOM).\nWith the steps above, calls to methods like llm.prompt will offload computation to your GPU and speed up responses from the LLM."
  },
  {
    "objectID": "index.html#faq",
    "href": "index.html#faq",
    "title": "OnPrem.LLM",
    "section": "FAQ",
    "text": "FAQ\n\nHow do I use other models with OnPrem.LLM?\n\nYou can supply the URL to other models to the LLM constructor, as we did above in the code generation example.\n\n\nWe currently support models in GGML format. However, the GGML format has now been superseded by GGUF. As of August 21st 2023, llama.cpp no longer supports GGML models, which is why we are pinning to an older version of all dependencies.\nFuture versions of OnPrem.LLM will use the newer GGUF format.\n\nI’m behind a corporate firewall and am receiving an SSL error when trying to download the model?\n\nTry this:\nfrom onprem import LLM\nLLM.download_model(url, ssl_verify=False)\n\nHow do I use this on a machine with no internet access?\n\nUse the LLM.download_model method to download the model files to &lt;your_home_directory&gt;/onprem_data and transfer them to the same location on the air-gapped machine.\n\n\nFor the ingest and ask methods, you will need to also download and transfer the embedding model files:\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\nmodel.save('/some/folder')\n\n\nCopy the some/folder folder to the air-gapped machine and supply the path to LLM via the embedding_model parameter.\n\nWhen installing onprem, I’m getting errors related to llama-cpp-python on Windows/Mac/Linux?\n\nFor Linux systems like Ubuntu, try this: sudo apt-get install build-essential g++ clang. Other tips are here.\n\n\nFor Windows systems, either use Windows Subsystem for Linux (WSL) or install Microsoft Visual Studio build tools and ensure the selections shown in this post are installed. WSL is recommended.\n\n\nFor Macs, try following these tips.\n\n\nIf you still have problems, there are various other tips for each of the above OSes in this privateGPT repo thread. Of course, you can also easily use OnPrem.LLM on Google Colab."
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nLLM\n\n LLM (model_url='https://huggingface.co/TheBloke/Wizard-\n      Vicuna-7B-Uncensored-GGML/resolve/main/Wizard-\n      Vicuna-7B-Uncensored.ggmlv3.q4_0.bin', use_larger=False,\n      n_gpu_layers:Optional[int]=None,\n      model_download_path:Optional[str]=None,\n      vectordb_path:Optional[str]=None, max_tokens:int=512,\n      n_ctx:int=2048, n_batch:int=1024, mute_stream=False,\n      embedding_model_name:str='sentence-transformers/all-MiniLM-L6-v2',\n      embedding_model_kwargs:dict={'device': 'cpu'}, confirm=True,\n      verbose=False, **kwargs)\n\nLLM Constructor. Extra kwargs are fed directly to langchain.llms.LlamaCpp.\nArgs:\n\nmodel_url: URL to .bin model (currently must be GGML model).\n*use_larger**: If True, a larger model than the default model_url will be used.\nn_gpu_layers: Number of layers to be loaded into gpu memory. Default is None.\nmodel_download_path: Path to download model. Default is onprem_data in user’s home directory.\nvectordb_path: Path to vector database (created if it doesn’t exist). Default is onprem_data/vectordb in user’s home directory.\nmax_tokens: The maximum number of tokens to generate.\nn_ctx: Token context window.\nn_batch: Number of tokens to process in parallel.\nmute_stream: Mute ChatGPT-like token stream output during generation\nembedding_model: name of sentence-transformers model. Used for LLM.ingest and LLM.ask.\nembedding_model_kwargs: arguments to embedding model (e.g., {device':'cpu'}).\nconfirm: whether or not to confirm with user before downloading a model\nverbose: Verbosity\n\n\nsource\n\n\nLLM.download_model\n\n LLM.download_model (model_url='https://huggingface.co/TheBloke/Wizard-\n                     Vicuna-7B-Uncensored-GGML/resolve/main/Wizard-\n                     Vicuna-7B-Uncensored.ggmlv3.q4_0.bin',\n                     model_download_path:Optional[str]=None, confirm=True,\n                     ssl_verify=True)\n\nDownload an LLM in GGML format supported by lLama.cpp.\nArgs:\n\nmodel_url: URL of model\nmodel_download_path: Path to download model. Default is onprem_data in user’s home directory.\nconfirm: whether or not to confirm with user before downloading\nssl_verify: If True, SSL certificates are verified. You can set to False if corporate firewall gives you problems.\n\n\nllm = LLM(DEFAULT_MODEL_URL, confirm=False)\n\n\nassert os.path.isfile(os.path.join(U.get_datadir(), os.path.basename(DEFAULT_MODEL_URL)))\n\n\nsource\n\n\nLLM.prompt\n\n LLM.prompt (prompt, prompt_template=None)\n\nSend prompt to LLM to generate a response\n\nprompt = \"\"\"Extract the names of people in the supplied sentences. Here is an example:\nSentence: James Gandolfini and Paul Newman were great actors.\nPeople:\nJames Gandolfini, Paul Newman\nSentence:\nI like Cillian Murphy's acting. Florence Pugh is great, too.\nPeople:\"\"\"\n\n\nsaved_output = llm.prompt(prompt)\n\nggml_init_cublas: found 2 CUDA devices:\n  Device 0: NVIDIA TITAN V, compute capability 7.0\n  Device 1: NVIDIA TITAN V, compute capability 7.0\nllama.cpp: loading model from /home/amaiya/onprem_data/Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal: n_embd     = 4096\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 32\nllama_model_load_internal: n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx size =    0.08 MB\nllama_model_load_internal: using CUDA for GPU acceleration\nggml_cuda_set_main_device: using device 0 (NVIDIA TITAN V) as main device\nllama_model_load_internal: mem required  = 5407.72 MB (+ 1026.00 MB per state)\nllama_model_load_internal: offloading 0 repeating layers to GPU\nllama_model_load_internal: offloaded 0/35 layers to GPU\nllama_model_load_internal: total VRAM used: 384 MB\nllama_new_context_with_model: kv self size  = 1024.00 MB\n\n\n\nCillian Murphy, Florence Pugh\n\n\n\nsource\n\n\nLLM.load_ingester\n\n LLM.load_ingester ()\n\nGet Ingester instance. You can access the langchain.vectorstores.Chroma instance with load_ingester().get_db().\n\nsource\n\n\nLLM.ingest\n\n LLM.ingest (source_directory:str)\n\nIngests all documents in source_folder into vector database. Previously-ingested documents are ignored.\nArgs:\n\nsource_directory: path to folder containing document store\n\nReturns: None\n\nllm.ingest('./sample_data')\n\n2023-09-04 11:57:49.836585: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nLoading new documents: 100%|██████████████████████| 2/2 [00:00&lt;00:00, 15.78it/s]\n\n\nCreating new vectorstore\nLoading documents from ./sample_data\nLoaded 11 new documents from ./sample_data\nSplit into 62 chunks of text (max. 500 tokens each)\nCreating embeddings. May take some minutes...\nIngestion complete! You can now query your documents using the LLM.ask method\n\n\n\nsource\n\n\nLLM.ask\n\n LLM.ask (question, num_source_docs=4)\n\nAnswer a question based on source documents fed to the ingest method.\nArgs:\n\nquestion: a question you want to ask\nnum_source_docs: the number of ingested source documents use to generate answer\n\n\nquestion = \"\"\"What is ktrain? Remember to only use the provided context?\"\"\" \nanswer, docs = llm.ask(question)\nprint('\\n\\nReferences:\\n\\n')\nfor i, document in enumerate(docs):\n    print(f\"\\n{i+1}.&gt; \" + document.metadata[\"source\"] + \":\")\n    print(document.page_content)\n\n Ktrain is a low-code ML library designed to aid in the development of machine learning models. It provides automation for certain aspects of the workow, while still allowing users to make choices that best fit their unique application requirements. By combining the strengths of both human engineers and automated processes, ktrain seeks to augment and complement rather than completely replace human expertise.\n\nReferences:\n\n\n\n1.&gt; ./sample_data/ktrain_paper.pdf:\nlection (He et al., 2019). By contrast, ktrain places less emphasis on this aspect of au-\ntomation and instead focuses on either partially or fully automating other aspects of the\nmachine learning (ML) workﬂow. For these reasons, ktrain is less of a traditional Au-\n2\n\n2.&gt; ./sample_data/ktrain_paper.pdf:\npossible, ktrain automates (either algorithmically or through setting well-performing de-\nfaults), but also allows users to make choices that best ﬁt their unique application require-\nments. In this way, ktrain uses automation to augment and complement human engineers\nrather than attempting to entirely replace them. In doing so, the strengths of both are\nbetter exploited. Following inspiration from a blog post1 by Rachel Thomas of fast.ai\n\n3.&gt; ./sample_data/ktrain_paper.pdf:\nwith custom models and data formats, as well.\nInspired by other low-code (and no-\ncode) open-source ML libraries such as fastai (Howard and Gugger, 2020) and ludwig\n(Molino et al., 2019), ktrain is intended to help further democratize machine learning by\nenabling beginners and domain experts with minimal programming or data science experi-\n4. http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\n6\n\n4.&gt; ./sample_data/ktrain_paper.pdf:\nktrain: A Low-Code Library for Augmented Machine Learning\ntoML platform and more of what might be called a “low-code” ML platform. Through\nautomation or semi-automation, ktrain facilitates the full machine learning workﬂow from\ncurating and preprocessing inputs (i.e., ground-truth-labeled training data) to training,\ntuning, troubleshooting, and applying models. In this way, ktrain is well-suited for domain\nexperts who may have less experience with machine learning and software coding. Where\n\n\nPro-Tip: If you see the model hallucinating answers, you can supply use_larger=True to LLM and use the slightly larger default model better-suited to this use case (or supply the URL to a different model of your choosing to LLM), which can provide better performance."
  }
]